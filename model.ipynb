{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Google drive mounting\n",
    "from google.colab import auth\n",
    "from pydrive2.auth import GoogleAuth\n",
    "from pydrive2.drive import GoogleDrive\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "#file download from the google drive\n",
    "# folder id\n",
    "folder_id = '13HMnXVHuRSz7HxZn80gh_N7VEWC4TlLY'\n",
    "\n",
    "# downloading of the file\n",
    "file_list = drive.ListFile({'q': f\"'{folder_id}' in parents and trashed=false\"}).GetList()\n",
    "\n",
    "for file in file_list:\n",
    "    print(f'Downloading {file[\"title\"]}...')\n",
    "    file.GetContentFile(file['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data preprocessing\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive',force_remount='True')\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "# Assuming you have already authenticated and created the PyDrive client\n",
    "# Authenticate and create the PyDrive client (if not already done)\n",
    "from google.colab import auth\n",
    "from pydrive2.auth import GoogleAuth\n",
    "from pydrive2.drive import GoogleDrive\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "# Replace 'your_folder_id' with the actual folder ID of your EEG data folder\n",
    "folder_id = '13HMnXVHuRSz7HxZn80gh_N7VEWC4TlLY'\n",
    "\n",
    "# List and download files from the folder\n",
    "file_list = drive.ListFile({'q': f\"'{folder_id}' in parents and trashed=false\"}).GetList()\n",
    "\n",
    "data_frames = []\n",
    "for file in file_list:\n",
    "    df = pd.read_csv(file['title'])\n",
    "    data_frames.append(df)\n",
    "\n",
    "# Combine data from all files\n",
    "combined_data = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the combined data\n",
    "print(combined_data.head())\n",
    "\n",
    "# Assuming the EEG data has columns for time and channels like Fp1, Fp2, etc.\n",
    "# Exclude the time column and convert to numeric\n",
    "data = combined_data.iloc[:, 1:].apply(pd.to_numeric, errors='coerce').values.T  # Transpose to get channels as rows\n",
    "ch_names = combined_data.columns[1:].tolist()  # Convert to list\n",
    "info = mne.create_info(ch_names=ch_names, sfreq=256, ch_types='eeg')  # Adjust sfreq as needed\n",
    "raw = mne.io.RawArray(data, info)\n",
    "\n",
    "# Preprocessing steps\n",
    "\n",
    "# 1. Filtering (e.g., 1-50 Hz band-pass filter)\n",
    "raw.filter(1., 50., fir_design='firwin')\n",
    "\n",
    "# 2. Notch filter to remove power line noise (e.g., 50 Hz in Europe, 60 Hz in the US)\n",
    "raw.notch_filter(freqs=50)\n",
    "\n",
    "# Save the preprocessed data to a CSV file\n",
    "preprocessed_data_df = pd.DataFrame(data.T, columns=ch_names)\n",
    "csv_path = '/content/drive/MyDrive/preprocessed_eeg_data.csv'\n",
    "preprocessed_data_df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Preprocessing complete. Data saved as 'preprocessed_eeg_data.fif' and 'preprocessed_eeg_data.csv' in Google Drive at {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature extraction\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from scipy.signal import welch\n",
    "\n",
    "# Path to the CSV file in Google Drive\n",
    "csv_path = '/content/drive/MyDrive/preprocessed_eeg_data.csv'\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "# Display the first few rows of the loaded data\n",
    "print(data.head())\n",
    "\n",
    "\n",
    "# Define frequency bands\n",
    "bands = {\n",
    "    'delta': (0.5, 4),\n",
    "    'theta': (4, 8),\n",
    "    'alpha': (8, 12),\n",
    "    'beta': (12, 30),\n",
    "    'gamma': (30, 45)\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store the features\n",
    "features = {ch: {} for ch in raw.ch_names}\n",
    "\n",
    "# Calculate Power Spectral Density (PSD) for each channel\n",
    "data_array = raw.get_data()\n",
    "psds = []\n",
    "freqs = None\n",
    "for ch_data in data_array:\n",
    "    f, pxx = welch(ch_data, fs=raw.info['sfreq'], nperseg=256)\n",
    "    psds.append(pxx)\n",
    "    if freqs is None:\n",
    "        freqs = f\n",
    "psds = np.array(psds)\n",
    "\n",
    "# Extract band power for each frequency band\n",
    "for ch_idx, ch_name in enumerate(raw.ch_names):\n",
    "    for band, (low, high) in bands.items():\n",
    "        # Select the frequency range for the current band\n",
    "        freq_mask = (freqs >= low) & (freqs <= high)\n",
    "        # Calculate the band power using the trapezoidal rule\n",
    "        band_power = np.trapz(psds[ch_idx][freq_mask], freqs[freq_mask])\n",
    "        features[ch_name][f'{band}_power'] = band_power\n",
    "\n",
    "# Extract statistical features (mean, variance)\n",
    "for ch_idx, ch_name in enumerate(raw.ch_names):\n",
    "    features[ch_name]['mean'] = np.mean(data_array[ch_idx])\n",
    "    features[ch_name]['variance'] = np.var(data_array[ch_idx])\n",
    "\n",
    "# Convert the features dictionary to a DataFrame for easier analysis\n",
    "feature_df = pd.DataFrame(features).T\n",
    "print(feature_df)\n",
    "\n",
    "# Save the features DataFrame to a CSV file in Google Drive\n",
    "feature_csv_path = '/content/drive/MyDrive/eeg_features.csv'\n",
    "feature_df.to_csv(feature_csv_path, index=True)\n",
    "\n",
    "print(f\"Feature extraction complete. Features saved as 'eeg_features.csv' in Google Drive at {feature_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model training from generative adversal network and auto encoder\n",
    "# Import necessary libraries\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the preprocessed EEG data from CSV file\n",
    "csv_path = '/content/drive/MyDrive/preprocessed_eeg_data.csv'\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "# Display the first few rows of the loaded data\n",
    "print(data.head())\n",
    "\n",
    "# Assuming the EEG data has columns for time and channels like Fp1, Fp2, etc.\n",
    "# Exclude the time column and convert to numeric\n",
    "data_values = data.iloc[:, 1:].apply(pd.to_numeric, errors='coerce').values.T\n",
    "ch_names = data.columns[1:].tolist()\n",
    "info = mne.create_info(ch_names=ch_names, sfreq=256, ch_types='eeg')\n",
    "raw = mne.io.RawArray(data_values, info)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data_values.T).T\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "df = pd.DataFrame(data_scaled.T, columns=ch_names)\n",
    "\n",
    "# Autoencoder architecture\n",
    "input_dim = df.shape[1]\n",
    "encoding_dim = 64  # Adjust based on your requirement\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoder = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "decoder = Dense(input_dim, activation='sigmoid')(encoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(df, df, epochs=25, batch_size=32, shuffle=True)\n",
    "\n",
    "# Encode the data\n",
    "encoded_data = autoencoder.predict(df)\n",
    "\n",
    "# GAN architecture\n",
    "def build_generator(input_dim, output_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(output_dim, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def build_discriminator(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "input_dim_gan = 100\n",
    "output_dim_gan = df.shape[1]\n",
    "\n",
    "generator = build_generator(input_dim_gan, output_dim_gan)\n",
    "discriminator = build_discriminator(output_dim_gan)\n",
    "discriminator.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "discriminator.trainable = False\n",
    "gan_input = Input(shape=(input_dim_gan,))\n",
    "gen_out = generator(gan_input)\n",
    "gan_out = discriminator(gen_out)\n",
    "gan = Model(gan_input, gan_out)\n",
    "gan.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy')\n",
    "\n",
    "# Training the GAN\n",
    "epochs = 5000\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train discriminator\n",
    "    idx = np.random.randint(0, df.shape[0], batch_size)\n",
    "    real_samples = df.values[idx]\n",
    "    noise = np.random.normal(0, 1, (batch_size, input_dim_gan))\n",
    "    generated_samples = generator.predict(noise)\n",
    "\n",
    "    d_loss_real = discriminator.train_on_batch(real_samples, np.ones((batch_size, 1)))\n",
    "    d_loss_fake = discriminator.train_on_batch(generated_samples, np.zeros((batch_size, 1)))\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # Train generator\n",
    "    noise = np.random.normal(0, 1, (batch_size, input_dim_gan))\n",
    "    valid_y = np.ones((batch_size, 1))\n",
    "    g_loss = gan.train_on_batch(noise, valid_y)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"{epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]}%] [G loss: {g_loss}]\")\n",
    "\n",
    "print(\"GAN training complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
